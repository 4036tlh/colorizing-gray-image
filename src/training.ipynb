{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFucU9+sOMQIZtNSOMEIXh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"A77aiJkYR0S1"},"source":["import os\r\n","import sys\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import gc\r\n","import tensorflow as tf\r\n","keras = tf.keras\r\n","\r\n","from keras.layers import Conv2D, MaxPool2D, BatchNormalization, LeakyReLU, Concatenate, Activation, Input\r\n","from keras.layers import Conv2DTranspose as Deconv2D\r\n","from keras.models import Model\r\n","\r\n","import cv2\r\n","\r\n","################################################################# FUNCTION DECLARATION\r\n","\r\n","s = tf.compat.v1.InteractiveSession()\r\n","\r\n","def input_layer(n, input):\r\n","    layer = Conv2D(n, (3, 3), padding='same')(input)\r\n","    layer = BatchNormalization()(layer)\r\n","    layer = LeakyReLU(alpha=0.2)(layer)\r\n","    layer = Conv2D(n, (3, 3), strides=1, padding='same')(layer)\r\n","    layer = BatchNormalization()(layer)\r\n","    layer = LeakyReLU(alpha=0.2)(layer)\r\n","    \r\n","    return layer\r\n","    \r\n","def maxPool(n, input):\r\n","    layer = MaxPool2D((2, 2), strides=2)(input)\r\n","    \r\n","    for i in range(2):\r\n","        layer = Conv2D(n, (3, 3), padding='same')(layer)\r\n","        layer = BatchNormalization()(layer)\r\n","        layer = LeakyReLU(alpha=0.2)(layer)\r\n","    \r\n","    return layer\r\n","    \r\n","def upConv(n, input_1, input_2):\r\n","    layer = Deconv2D(n, (2, 2), strides=2)(input_2)\r\n","    layer = Concatenate()([input_1, layer])\r\n","    \r\n","    for i in range(2):\r\n","        layer = Conv2D(n, (3, 3), padding='same')(layer)\r\n","        layer = BatchNormalization()(layer)\r\n","        layer = Activation('relu')(layer)\r\n","    \r\n","    return layer\r\n","\r\n","#define the model\r\n","def UNet(x_shape):\r\n","    input = Input(x_shape)\r\n","    \r\n","    #convolution\r\n","    X0 = input_layer(64, input)\r\n","    X1 = maxPool(128, X0)\r\n","    X2 = maxPool(256, X1)\r\n","    X3 = maxPool(512, X2)\r\n","    X4 = maxPool(1024, X3)\r\n","    \r\n","    #up convolution\r\n","    X4 = upConv(512, X3, X4)\r\n","    X3 = upConv(256, X2, X4)\r\n","    X2 = upConv(128, X1, X3)\r\n","    X1 = upConv(64, X0, X2)\r\n","    \r\n","    #to 3 channel\r\n","    X0 = Conv2D(3, (1, 1), strides=1)(X1)\r\n","    \r\n","    #define the output model\r\n","    model = Model(inputs=input, outputs=X0)\r\n","    \r\n","    return model\r\n","\r\n","#load the previous training history\r\n","def load_previous_epoch(path_history):\r\n","    file = open(path_history, \"r\")\r\n","    previous_epoch = 0\r\n","  \r\n","    CoList = file.read().split(\"\\n\") \r\n","    \r\n","    file.close()\r\n","    for i in CoList: \r\n","        if i: \r\n","            previous_epoch += 1\r\n","    return previous_epoch\r\n","  \r\n","#testing and show the images\r\n","def test_sample(color, gray, w, e):\r\n","    output = model.predict(gray)\r\n","    \r\n","    #convert from BGR to RGB\r\n","    output = output[...,::-1]\r\n","    color = color[...,::-1]\r\n","    gray = gray[...,::-1]\r\n","    \r\n","    n=1\r\n","    for i in range(3):\r\n","        plt.subplot(3, 3, n)\r\n","        plt.imshow(color[i].reshape((w, w, 3)), cmap=\"gray\", interpolation='none')\r\n","        plt.subplot(3, 3, n+1)\r\n","        plt.imshow(gray[i].reshape((w, w)), cmap=\"gray\", interpolation='none')\r\n","        plt.subplot(3, 3, n+2)\r\n","        plt.imshow(output[i].reshape((w, w, 3)), cmap=\"gray\", interpolation='none')\r\n","        n += 3\r\n","    \r\n","    #save_test_path = '/content/gdrive/My Drive/colorization/256/img/Epoch_'+str(e)+'.jpg'\r\n","    #plt.savefig(save_test_path)\r\n","    plt.show()\r\n","\r\n","#read images and convert to array form\r\n","def batch_generator():\r\n","    images = []\r\n","    \r\n","    train_list = random.sample(range(TOTAL_TRAIN), TRAIN_IMAGES_PER_EPOCH)\r\n","    test_list = random.sample(range(TOTAL_TEST), TEST_IMAGES_PER_EPOCH)\r\n","    t_load = TRAIN_IMAGES_PER_EPOCH + TEST_IMAGES_PER_EPOCH\r\n","    path_list = train_list + test_list\r\n","    \r\n","    i = 0\r\n","    for id in train_list:\r\n","        filename = img_path_train[id]\r\n","        img = cv2.imread(filename)\r\n","                                  \r\n","        img=cv2.resize(img,(w,w))\r\n","        images.append(img)\r\n","      \r\n","        sys.stdout.write(\"\\rLoading training img {}/{}\".format(i+1, TRAIN_IMAGES_PER_EPOCH))\r\n","        sys.stdout.flush()\r\n","    \r\n","        i = i + 1\r\n","      \r\n","    i = 0\r\n","    for id in test_list:\r\n","        filename = img_path_train[id]\r\n","        img = cv2.imread(filename)\r\n","                                  \r\n","        img=cv2.resize(img,(w,w))\r\n","        images.append(img)\r\n","      \r\n","        sys.stdout.write(\"\\rLoading testing img {}/{}\".format(i+1, TEST_IMAGES_PER_EPOCH))\r\n","        sys.stdout.flush()\r\n","    \r\n","        i = i + 1\r\n","\r\n","    images = np.array(images)\r\n","    images = images/255.\r\n","\r\n","    gray_images = np.mean(images, axis=-1)\r\n","    gray_images = gray_images.reshape((*gray_images.shape, 1))\r\n","\r\n","    return images[:TRAIN_IMAGES_PER_EPOCH], gray_images[:TRAIN_IMAGES_PER_EPOCH], images[-TEST_IMAGES_PER_EPOCH:], gray_images[-TEST_IMAGES_PER_EPOCH:]\r\n","   \r\n","############################################################################# DEFINE PARAMETERS\r\n","EPOCH = 2000\r\n","BATCH_SIZE = 16\r\n","TRAIN_IMAGES_PER_EPOCH = 2048\r\n","TEST_IMAGES_PER_EPOCH = 512\r\n","load_PreTrain = True\r\n","\r\n","w = 256\r\n","\r\n","############################################################################ LOADING PREVIOUS INFORMATION\r\n","x_shape = (w,w,1)\r\n","model = UNet(x_shape)\r\n","\r\n","model.compile('adam', loss='mean_squared_error', metrics=['mae', 'acc'])\r\n","#model.summary()\r\n","\r\n","if load_PreTrain:\r\n","    model.load_weights(\"./model.hdf5\")\r\n","    previous_epoch = load_previous_epoch('./history.txt')\r\n","    \r\n","else:\r\n","    previous_epoch = 0\r\n","\r\n","print('Start from Epoch : {}'.format(previous_epoch))\r\n","\r\n","############################################################################# TRAINING\r\n","for epoch in range(previous_epoch, previous_epoch+EPOCH):\r\n","\r\n","    print('EPOCH : {}'.format(epoch))\r\n","\r\n","    Y_train, X_train, Y_test, X_test = batch_generator()\r\n","\r\n","    hist = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, validation_data=(X_test, Y_test))\r\n","    \r\n","    record = (\"{} {} {} {} {} {}\\n\".format(hist.history['loss'][0],hist.history['mae'][0],hist.history['acc'][0],hist.history['val_loss'][0],hist.history['val_mae'][0],hist.history['val_acc'][0]))\r\n","    \r\n","    with open(\"./history.txt\", \"a\") as file_object:\r\n","        file_object.write(record)\r\n","\r\n","    model.save('./model.hdf5')\r\n","    \r\n","    test_sample(Y_test[:3], X_test[:3], w, epoch)"],"execution_count":null,"outputs":[]}]}